---
title: "注意力机制：从直觉到 Transformer 架构"

description: "深入理解注意力机制的工作原理，探索从简单注意力到自注意力机制的演进，以及 Transformer 如何革命性地改变了深度学习"

pubDate: 2024-12-03

published: true
---

# 注意力机制：从直觉到 Transformer 架构

注意力机制的提出彻底改变了深度学习的发展轨迹。从解决序列到序列模型中的信息瓶颈问题开始，到 Transformer 架构的提出，注意力机制不仅解决了长序列建模的难题，更开启了大规模预训练模型的新纪元。

## 动机：为什么需要注意力机制？

在传统的编码器-解码器架构中，编码器将整个输入序列压缩成一个固定长度的向量表示。这种方法存在明显的信息瓶颈：

![Seq2Seq Architecture](https://miro.medium.com/v2/resize:fit:1400/1*1JcHGUU7rFgtXC_mydUA_Q.jpeg)

当输入序列很长时，固定长度的上下文向量难以保存所有重要信息，导致模型性能下降。这就像让一个人记住一整本书的内容，然后仅凭记忆来回答问题——显然不现实。

## 注意力机制的核心思想

注意力机制的基本思想是：在生成每个输出时，模型可以"注意"到输入序列的不同部分，而不是仅仅依赖最后一个隐状态。

### 数学表述

给定查询 $Q$、键 $K$ 和值 $V$，注意力函数计算为：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

其中：
- $Q \in \mathbb{R}^{n \times d_k}$ 是查询矩阵
- $K \in \mathbb{R}^{m \times d_k}$ 是键矩阵  
- $V \in \mathbb{R}^{m \times d_v}$ 是值矩阵
- $d_k$ 是键的维度，用于缩放防止 softmax 饱和

### 直觉理解

可以将注意力机制理解为一个软查找表：
1. **查询**（Query）：我想要什么信息？
2. **键**（Key）：每个位置提供什么类型的信息？
3. **值**（Value）：每个位置的实际信息内容是什么？

注意力权重 $\alpha_{ij} = \text{softmax}(\text{score}(q_i, k_j))$ 表示第 $i$ 个查询对第 $j$ 个键-值对的关注程度。

## 从基础注意力到自注意力

### 1. 加性注意力（Additive Attention）

Bahdanau 等人提出的最早期注意力机制：

$$
\text{score}(h_i, s_j) = v^T \tanh(W_h h_i + W_s s_j)
$$

```python showLineNumbers
def additive_attention(query, keys, values):
    """
    加性注意力实现
    query: (batch_size, hidden_size)
    keys: (batch_size, seq_len, hidden_size)  
    values: (batch_size, seq_len, hidden_size)
    """
    # 计算注意力分数
    expanded_query = query.unsqueeze(1).expand(-1, keys.size(1), -1)
    scores = torch.tanh(
        torch.linear(expanded_query, W_q) + 
        torch.linear(keys, W_k)
    )
    scores = torch.linear(scores, v)
    
    # 应用 softmax
    attention_weights = F.softmax(scores, dim=1)
    
    # 计算上下文向量
    context = torch.sum(attention_weights.unsqueeze(-1) * values, dim=1)
    return context, attention_weights
````

### 2. 缩放点积注意力（Scaled Dot-Product Attention）

这是 Transformer 中使用的核心注意力机制：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

![Scaled Dot-Product Attention](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    缩放点积注意力
    Q, K, V: (batch_size, seq_len, d_model)
    """
    d_k = Q.size(-1)
    
    # 计算注意力分数
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    
    # 应用掩码（如果提供）
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # Softmax
    attention_weights = F.softmax(scores, dim=-1)
    
    # 应用注意力权重
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
```

## 多头注意力：并行处理不同类型信息

单一注意力头可能无法捕获所有类型的依赖关系。多头注意力允许模型同时关注不同的信息子空间：

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
$$

其中：

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

![Multi-Head Attention](https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png)

### 多头注意力的优势

1. **信息子空间**：不同的头可以关注不同类型的关系（语法、语义、长距离依赖等）
2. **并行计算**：各个头可以并行计算，提高效率
3. **表示丰富性**：组合多个头的输出提供更丰富的表示

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # 线性变换并重塑为多头形式
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # 应用缩放点积注意力
        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)
        
        # 重新组合多头输出
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # 最终线性变换
        output = self.W_o(attention_output)
        
        return output, attention_weights
```

## Transformer 架构：注意力的集大成者

Transformer 完全基于注意力机制，抛弃了循环结构，实现了真正的并行化训练。

![Transformer Architecture](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)

### 核心组件

#### 1. 位置编码（Positional Encoding）

由于注意力机制本身不包含位置信息，需要显式添加位置编码：

$$
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

$$
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

#### 2. 层归一化（Layer Normalization）

在每个子层后应用层归一化，加速训练并提高稳定性：

$$
\text{LayerNorm}(x) = \gamma \frac{x - \mu}{\sigma} + \beta
$$

#### 3. 残差连接（Residual Connection）

每个子层都采用残差连接：

$$
\text{Output} = \text{LayerNorm}(x + \text{Sublayer}(x))
$$

### 完整的 Transformer 编码器层

```python
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Linear(d_ff, d_model)
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # 自注意力子层
        attention_output, _ = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attention_output))
        
        # 前馈网络子层
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x
```

## 注意力模式可视化

不同类型的注意力模式反映了模型学到的不同类型关系：

### 1. 语法依赖

某些注意力头专门捕获语法结构，如主谓关系、修饰关系等。

### 2. 共指消解

一些头学会将代词与其指代的实体连接起来。

### 3. 长距离依赖

某些头能够捕获跨越很长距离的语义关系。

## 注意力机制的变种与改进

### 1. 稀疏注意力（Sparse Attention）

为了处理更长的序列，研究者提出了各种稀疏注意力模式：

* **局部注意力**：只关注邻近位置
* **块状注意力**：将序列分块处理
* **随机注意力**：随机选择部分位置进行注意力计算

### 2. 线性注意力（Linear Attention）

通过核函数技巧将二次复杂度降低到线性：

$$
\text{Attention}(Q, K, V) = \phi(Q)(\phi(K)^T V)
$$

其中 $\phi$ 是正定核函数的特征映射。

### 3. 相对位置编码

不使用绝对位置，而是编码相对位置关系：

$$
e_{ij} = \frac{(x_i W_Q)(x_j W_K + r_{i-j})^T}{\sqrt{d}}
$$

